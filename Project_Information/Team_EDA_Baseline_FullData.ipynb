{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b20d8c",
   "metadata": {},
   "source": [
    "# Team EDA Baseline (Full Data + R2 Download)\n",
    "\n",
    "This notebook starts implementation for shared team EDA with the full Stage-1 dataset.\n",
    "\n",
    "Scope:\n",
    "- Validate `scripts/r2.env`\n",
    "- Run `scripts/download_from_r2.sh`\n",
    "- Verify downloaded files against manifest\n",
    "- Load sample parquet partitions for sanity checks before full EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048aae2c",
   "metadata": {},
   "source": [
    "## 1) Load Project Dependencies and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da18e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /Users/leonschmidt/Projekte/Machine_Learning_Spotify/Git_Project/ML_Group_AB\n",
      "Scripts dir: /Users/leonschmidt/Projekte/Machine_Learning_Spotify/Git_Project/ML_Group_AB/scripts\n",
      "Default download root: /Users/leonschmidt/Projekte/Machine_Learning_Spotify/Git_Project/ML_Group_AB/dataset_downloads/v1\n",
      "R2 env file: /Users/leonschmidt/Projekte/Machine_Learning_Spotify/Git_Project/ML_Group_AB/scripts/r2.env\n",
      "Output dir: /Users/leonschmidt/Projekte/Machine_Learning_Spotify/Git_Project/ML_Group_AB/Project_Information/outputs/team_eda_baseline\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "if NOTEBOOK_DIR.name != \"Project_Information\":\n",
    "    for candidate in [Path.cwd(), *Path.cwd().parents]:\n",
    "        if (candidate / \"scripts\").exists() and (candidate / \"datasets\").exists():\n",
    "            NOTEBOOK_DIR = candidate / \"Project_Information\" if (candidate / \"Project_Information\").exists() else candidate\n",
    "            break\n",
    "\n",
    "REPO_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"Project_Information\" else NOTEBOOK_DIR\n",
    "SCRIPTS_DIR = REPO_ROOT / \"scripts\"\n",
    "DEFAULT_DATASET_VERSION = \"v1\"\n",
    "DEFAULT_DOWNLOAD_ROOT = REPO_ROOT / \"datasets\" / DEFAULT_DATASET_VERSION\n",
    "R2_ENV_PATH = SCRIPTS_DIR / \"r2.env\"\n",
    "R2_ENV_EXAMPLE_PATH = SCRIPTS_DIR / \"r2.env.example\"\n",
    "OUTPUT_DIR = NOTEBOOK_DIR / \"outputs\" / \"team_eda_baseline\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Repo root: {REPO_ROOT}\")\n",
    "print(f\"Scripts dir: {SCRIPTS_DIR}\")\n",
    "print(f\"Default download root: {DEFAULT_DOWNLOAD_ROOT}\")\n",
    "print(f\"R2 env file: {R2_ENV_PATH}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd3317",
   "metadata": {},
   "source": [
    "## 2) Parse and Validate `scripts/r2.env` Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b4f520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ r2.env validation passed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R2_ENDPOINT</th>\n",
       "      <td>https://a96b93c5d97cddb48fc674255fb687c7.r2.cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2_BUCKET</th>\n",
       "      <td>ml-group-ab-datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWS_ACCESS_KEY_ID</th>\n",
       "      <td>fa162d7d8a7a374608e16c3858dfb6f6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWS_SECRET_ACCESS_KEY</th>\n",
       "      <td>c810c10fdc699b06aa6d561f1d052387ec79771ea972db...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATASET_VERSION</th>\n",
       "      <td>v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   value\n",
       "R2_ENDPOINT            https://a96b93c5d97cddb48fc674255fb687c7.r2.cl...\n",
       "R2_BUCKET                                           ml-group-ab-datasets\n",
       "AWS_ACCESS_KEY_ID                       fa162d7d8a7a374608e16c3858dfb6f6\n",
       "AWS_SECRET_ACCESS_KEY  c810c10fdc699b06aa6d561f1d052387ec79771ea972db...\n",
       "DATASET_VERSION                                                       v1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "REQUIRED_R2_KEYS = [\n",
    "    \"R2_ENDPOINT\",\n",
    "    \"R2_BUCKET\",\n",
    "    \"AWS_ACCESS_KEY_ID\",\n",
    "    \"AWS_SECRET_ACCESS_KEY\",\n",
    "    \"DATASET_VERSION\",\n",
    "]\n",
    "\n",
    "\n",
    "def _strip_balanced_quotes(value: str) -> str:\n",
    "    if len(value) >= 2 and ((value[0] == '\"' and value[-1] == '\"') or (value[0] == \"'\" and value[-1] == \"'\")):\n",
    "        return value[1:-1]\n",
    "    return value\n",
    "\n",
    "\n",
    "def parse_env_file(env_path: Path) -> Tuple[Dict[str, str], List[str]]:\n",
    "    parsed: Dict[str, str] = {}\n",
    "    errors: List[str] = []\n",
    "\n",
    "    if not env_path.exists():\n",
    "        errors.append(f\"Missing env file: {env_path}\")\n",
    "        return parsed, errors\n",
    "\n",
    "    for line_no, raw_line in enumerate(env_path.read_text(encoding=\"utf-8\").splitlines(), start=1):\n",
    "        line = raw_line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if \"=\" not in line:\n",
    "            errors.append(f\"Line {line_no}: missing '=' separator\")\n",
    "            continue\n",
    "\n",
    "        key, value = line.split(\"=\", 1)\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "\n",
    "        if not re.fullmatch(r\"[A-Za-z_][A-Za-z0-9_]*\", key):\n",
    "            errors.append(f\"Line {line_no}: invalid key '{key}'\")\n",
    "            continue\n",
    "\n",
    "        dbl_quote_count = value.count('\"')\n",
    "        sgl_quote_count = value.count(\"'\")\n",
    "        if dbl_quote_count % 2 != 0 or sgl_quote_count % 2 != 0:\n",
    "            errors.append(\n",
    "                f\"Line {line_no}: malformed quotes for key '{key}' -> {value!r}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        normalized = _strip_balanced_quotes(value).strip()\n",
    "\n",
    "        if normalized.endswith('\"') or normalized.endswith(\"'\"):\n",
    "            errors.append(\n",
    "                f\"Line {line_no}: value for '{key}' appears to have a trailing quote -> {value!r}\"\n",
    "            )\n",
    "\n",
    "        parsed[key] = normalized\n",
    "\n",
    "    missing = [k for k in REQUIRED_R2_KEYS if not parsed.get(k)]\n",
    "    if missing:\n",
    "        errors.append(f\"Missing required keys: {missing}\")\n",
    "\n",
    "    return parsed, errors\n",
    "\n",
    "\n",
    "r2_config, r2_config_errors = parse_env_file(R2_ENV_PATH)\n",
    "\n",
    "if r2_config_errors:\n",
    "    print(\"❌ r2.env validation failed:\")\n",
    "    for err in r2_config_errors:\n",
    "        print(f\"  - {err}\")\n",
    "    if R2_ENV_EXAMPLE_PATH.exists():\n",
    "        print(f\"\\nTemplate available at: {R2_ENV_EXAMPLE_PATH}\")\n",
    "else:\n",
    "    print(\"✅ r2.env validation passed\")\n",
    "    display(pd.DataFrame([r2_config]).T.rename(columns={0: \"value\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e5a7a",
   "metadata": {},
   "source": [
    "## 3) Implement Safe Environment Variable Export for Shell Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d12ab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Shell helper ready\n"
     ]
    }
   ],
   "source": [
    "def build_sanitized_env(base_env: Dict[str, str], overrides: Dict[str, str] | None = None) -> Dict[str, str]:\n",
    "    if r2_config_errors:\n",
    "        raise ValueError(\"Cannot build environment: r2.env is invalid.\")\n",
    "\n",
    "    env = dict(os.environ)\n",
    "    for key in REQUIRED_R2_KEYS:\n",
    "        env[key] = str(base_env[key]).strip()\n",
    "\n",
    "    if overrides:\n",
    "        for k, v in overrides.items():\n",
    "            if v is not None:\n",
    "                env[k] = str(v)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "def run_bash_script(script_path: Path, env: Dict[str, str], cwd: Path, timeout: int = 3600) -> subprocess.CompletedProcess:\n",
    "    if not script_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing script: {script_path}\")\n",
    "\n",
    "    cmd = [\"bash\", str(script_path)]\n",
    "    result = subprocess.run(\n",
    "        cmd,\n",
    "        cwd=str(cwd),\n",
    "        env=env,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "\n",
    "    print(\"--- stdout ---\")\n",
    "    print(result.stdout[-4000:] if result.stdout else \"<empty>\")\n",
    "    print(\"--- stderr ---\")\n",
    "    print(result.stderr[-4000:] if result.stderr else \"<empty>\")\n",
    "    print(f\"Exit code: {result.returncode}\")\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(\n",
    "            \"Download script failed. Check stdout/stderr above. \"\n",
    "            \"For permission-related preflight errors, try SKIP_R2_PREFLIGHT=1.\"\n",
    "        )\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"✅ Shell helper ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacca5d5",
   "metadata": {},
   "source": [
    "## 4) Run Dataset Download Script with Version Overrides\n",
    "\n",
    "This cell is safe by default (`RUN_DOWNLOAD = False`). Set it to `True` when ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef666d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_SCRIPT = SCRIPTS_DIR / \"download_from_r2.sh\"\n",
    "DATASET_VERSION = r2_config.get(\"DATASET_VERSION\", DEFAULT_DATASET_VERSION) if not r2_config_errors else DEFAULT_DATASET_VERSION\n",
    "DOWNLOAD_ROOT = REPO_ROOT / \"datasets\" / DATASET_VERSION\n",
    "\n",
    "RUN_DOWNLOAD = False\n",
    "RUN_OVERRIDE_EXAMPLE = False\n",
    "\n",
    "print(f\"Default DATASET_VERSION: {DATASET_VERSION}\")\n",
    "print(f\"Default DOWNLOAD_ROOT: {DOWNLOAD_ROOT}\")\n",
    "\n",
    "if RUN_DOWNLOAD:\n",
    "    env_default = build_sanitized_env(\n",
    "        r2_config,\n",
    "        overrides={\n",
    "            \"DATASET_VERSION\": DATASET_VERSION,\n",
    "            \"DOWNLOAD_ROOT\": str(DOWNLOAD_ROOT),\n",
    "        },\n",
    "    )\n",
    "    run_bash_script(DOWNLOAD_SCRIPT, env=env_default, cwd=REPO_ROOT)\n",
    "else:\n",
    "    print(\"Skipped default run. Set RUN_DOWNLOAD=True to execute.\")\n",
    "\n",
    "if RUN_OVERRIDE_EXAMPLE:\n",
    "    override_version = \"v1\"\n",
    "    override_root = REPO_ROOT / \"datasets\" / f\"{override_version}_override_demo\"\n",
    "    env_override = build_sanitized_env(\n",
    "        r2_config,\n",
    "        overrides={\n",
    "            \"DATASET_VERSION\": override_version,\n",
    "            \"DOWNLOAD_ROOT\": str(override_root),\n",
    "        },\n",
    "    )\n",
    "    run_bash_script(DOWNLOAD_SCRIPT, env=env_override, cwd=REPO_ROOT)\n",
    "else:\n",
    "    print(\"Skipped override example. Set RUN_OVERRIDE_EXAMPLE=True to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414d9201",
   "metadata": {},
   "source": [
    "## 5) Load Sample Parquet Partitions for Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645737fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'active_download_root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m full_root = \u001b[43mactive_download_root\u001b[49m / \u001b[33m\"\u001b[39m\u001b[33mfull\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m full_root.exists():\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFull dataset folder not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'active_download_root' is not defined"
     ]
    }
   ],
   "source": [
    "full_root = active_download_root / \"full\"\n",
    "if not full_root.exists():\n",
    "    raise FileNotFoundError(f\"Full dataset folder not found: {full_root}\")\n",
    "\n",
    "parquet_files = sorted(full_root.rglob(\"*.parquet\"))\n",
    "if not parquet_files:\n",
    "    raise FileNotFoundError(f\"No parquet files found under: {full_root}\")\n",
    "\n",
    "MAX_SAMPLE_FILES = 4\n",
    "sample_files = parquet_files[:MAX_SAMPLE_FILES]\n",
    "\n",
    "print(f\"Total parquet files in full dataset: {len(parquet_files)}\")\n",
    "print(f\"Sampling first {len(sample_files)} files for sanity checks\")\n",
    "for f in sample_files:\n",
    "    print(f\"  - {f.relative_to(active_download_root)}\")\n",
    "\n",
    "sample_df = pd.concat([pd.read_parquet(fp) for fp in sample_files], ignore_index=True)\n",
    "\n",
    "print(\"\\nSample dataframe shape:\", sample_df.shape)\n",
    "print(\"Sample dataframe memory usage (MB):\", round(sample_df.memory_usage(deep=True).sum() / 1_000_000, 2))\n",
    "\n",
    "null_ratio = (\n",
    "    sample_df.isna()\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .rename(\"null_ratio\")\n",
    "    .reset_index(names=\"column\")\n",
    ")\n",
    "\n",
    "display(sample_df.head(10))\n",
    "display(sample_df.dtypes.rename(\"dtype\").reset_index(names=\"column\").head(40))\n",
    "display(null_ratio.head(20))\n",
    "\n",
    "sample_df.head(200).to_csv(OUTPUT_DIR / \"sample_preview_200_rows.csv\", index=False)\n",
    "null_ratio.to_csv(OUTPUT_DIR / \"sample_null_ratio.csv\", index=False)\n",
    "\n",
    "print(f\"Saved sample preview to: {OUTPUT_DIR / 'sample_preview_200_rows.csv'}\")\n",
    "print(f\"Saved null ratio summary to: {OUTPUT_DIR / 'sample_null_ratio.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2120f6",
   "metadata": {},
   "source": [
    "## 6) Initial EDA Starter (Basic Team Baseline)\n",
    "\n",
    "This uses the sampled dataframe (`sample_df`) to provide quick baseline insights before full-scale EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11140d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_numeric = [c for c in [\"streams\", \"rank\", \"af_danceability\", \"af_energy\", \"af_valence\", \"af_tempo\"] if c in sample_df.columns]\n",
    "candidate_categorical = [c for c in [\"region\", \"chart\", \"artist\", \"title\", \"year\"] if c in sample_df.columns]\n",
    "\n",
    "print(\"Numeric columns used:\", candidate_numeric)\n",
    "print(\"Categorical columns used:\", candidate_categorical)\n",
    "\n",
    "if candidate_numeric:\n",
    "    display(sample_df[candidate_numeric].describe().T)\n",
    "\n",
    "for col in [c for c in [\"region\", \"chart\", \"year\"] if c in sample_df.columns]:\n",
    "    top_counts = sample_df[col].value_counts(dropna=False).head(15).rename(\"count\").reset_index(names=col)\n",
    "    print(f\"\\nTop values for {col}:\")\n",
    "    display(top_counts)\n",
    "    top_counts.to_csv(OUTPUT_DIR / f\"sample_top_{col}.csv\", index=False)\n",
    "\n",
    "if {\"year\", \"streams\"}.issubset(sample_df.columns):\n",
    "    yearly_streams = sample_df.groupby(\"year\", dropna=False)[\"streams\"].agg([\"count\", \"mean\", \"median\", \"sum\"]).reset_index()\n",
    "    display(yearly_streams)\n",
    "    yearly_streams.to_csv(OUTPUT_DIR / \"sample_yearly_streams_summary.csv\", index=False)\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if \"streams\" in sample_df.columns:\n",
    "        ax = sample_df[\"streams\"].dropna().clip(upper=sample_df[\"streams\"].quantile(0.99)).plot(\n",
    "            kind=\"hist\", bins=50, figsize=(8, 4), title=\"Streams distribution (clipped at 99th percentile)\"\n",
    "        )\n",
    "        ax.set_xlabel(\"streams\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "except Exception as exc:\n",
    "    print(f\"Plot skipped (matplotlib unavailable or plotting issue): {exc}\")\n",
    "\n",
    "print(f\"Saved starter EDA outputs to: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
